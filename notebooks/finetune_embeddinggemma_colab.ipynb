{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "# Installation avec version transformers recommandée par Google\n!pip install -q \"sentence-transformers[train]\" datasets accelerate huggingface_hub\n!pip install -q git+https://github.com/huggingface/transformers@v4.56.0-Embedding-Gemma-preview\n\nfrom google.colab import userdata\nfrom huggingface_hub import login\nlogin(token=userdata.get('HF_TOKEN'))",
   "metadata": {
    "id": "install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -q https://raw.githubusercontent.com/pierrealexandreguillemin-a11y/pocket_arbiter/main/data/training/triplets_training.jsonl\n",
    "import json\n",
    "triplets = [json.loads(l) for l in open(\"triplets_training.jsonl\") if l.strip()]\n",
    "print(f\"Triplets: {len(triplets)}\")"
   ],
   "metadata": {
    "id": "data"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom datasets import Dataset\n\n# Vérifier GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Charger le modèle EmbeddingGemma 300M\nmodel = SentenceTransformer(\"google/embeddinggemma-300M\", device=device)\nprint(f\"Model: {model.get_sentence_embedding_dimension()} dims\")\n\n# Configuration optimisée pour T4 16GB\n# IMPORTANT: EmbeddingGemma ne supporte PAS fp16 (voir Google docs)\n# batch_size=4 avec gradient_accumulation=4 = effective batch 16\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=SentenceTransformerTrainingArguments(\n        output_dir=\"embeddinggemma-chess-arbiter-fr\",\n        num_train_epochs=3,\n        per_device_train_batch_size=4,           # Réduit pour T4 (Google recommande 1)\n        gradient_accumulation_steps=4,            # Effective batch = 16\n        learning_rate=2e-5,\n        warmup_ratio=0.1,\n        fp16=False,                               # CRITIQUE: EmbeddingGemma ne supporte pas fp16!\n        bf16=False,                               # Pas de bf16 non plus sur T4\n        logging_steps=50,\n        save_strategy=\"epoch\",\n        report_to=\"none\",\n        dataloader_drop_last=True,                # Évite batch incomplet\n    ),\n    train_dataset=Dataset.from_list(triplets),\n    loss=MultipleNegativesRankingLoss(model)\n)\n\nprint(\"Training...\")\ntrainer.train()\nmodel.save(\"embeddinggemma-chess-arbiter-fr\")\nprint(\"Training terminé!\")",
   "metadata": {
    "id": "train"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Évaluation rapide du modèle fine-tuné (conformité ISO 42001)\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\nimport random\n\n# Charger le modèle fine-tuné\nfinetuned = SentenceTransformer(\"embeddinggemma-chess-arbiter-fr\")\n\n# Échantillon de test (5 triplets aléatoires)\ntest_samples = random.sample(triplets, min(5, len(triplets)))\n\nprint(\"=\" * 60)\nprint(\"ÉVALUATION QUALITÉ (ISO 42001 - AI-R03: Recall)\")\nprint(\"=\" * 60)\n\ncorrect = 0\nfor i, t in enumerate(test_samples):\n    q_emb = finetuned.encode(t[\"anchor\"])\n    pos_emb = finetuned.encode(t[\"positive\"])\n    neg_emb = finetuned.encode(t[\"negative\"])\n    \n    sim_pos = cos_sim(q_emb, pos_emb).item()\n    sim_neg = cos_sim(q_emb, neg_emb).item()\n    \n    is_correct = sim_pos > sim_neg\n    correct += int(is_correct)\n    \n    print(f\"\\n[{i+1}] Question: {t['anchor'][:60]}...\")\n    print(f\"    Sim(positive): {sim_pos:.4f}\")\n    print(f\"    Sim(negative): {sim_neg:.4f}\")\n    print(f\"    {'✅ CORRECT' if is_correct else '❌ INCORRECT'}\")\n\naccuracy = correct / len(test_samples) * 100\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"RÉSULTAT: {correct}/{len(test_samples)} = {accuracy:.0f}%\")\nprint(f\"Cible ISO: ≥80%  →  {'✅ CONFORME' if accuracy >= 80 else '⚠️ À AMÉLIORER'}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "shutil.make_archive(\"embeddinggemma-chess-arbiter-fr\", \"zip\", \"embeddinggemma-chess-arbiter-fr\")\n",
    "files.download(\"embeddinggemma-chess-arbiter-fr.zip\")"
   ],
   "metadata": {
    "id": "download"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}