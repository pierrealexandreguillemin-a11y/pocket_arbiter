{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "# Installation (rapide, pas besoin de version spéciale)\n!pip install -q sentence-transformers datasets accelerate",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Télécharger les triplets\n!wget -q https://raw.githubusercontent.com/pierrealexandreguillemin-a11y/pocket_arbiter/main/data/training/triplets_training.jsonl\n\nimport json\ntriplets = [json.loads(l) for l in open(\"triplets_training.jsonl\") if l.strip()]\nprint(f\"Triplets: {len(triplets)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom datasets import Dataset\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# MiniLM = 22M params (vs 308M pour EmbeddingGemma)\n# Fonctionne EASY sur T4\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\nprint(f\"Model: {model.get_sentence_embedding_dimension()} dims\")\n\n# Config généreuse - MiniLM est petit\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    args=SentenceTransformerTrainingArguments(\n        output_dir=\"minilm-chess-arbiter-fr\",\n        num_train_epochs=5,                       # Plus d'epochs car petit modèle\n        per_device_train_batch_size=64,           # Gros batch OK!\n        learning_rate=2e-5,\n        warmup_ratio=0.1,\n        fp16=True,                                # MiniLM supporte fp16\n        logging_steps=20,\n        save_strategy=\"epoch\",\n        report_to=\"none\",\n    ),\n    train_dataset=Dataset.from_list(triplets),\n    loss=MultipleNegativesRankingLoss(model)\n)\n\nprint(\"Training...\")\ntrainer.train()\nmodel.save(\"minilm-chess-arbiter-fr\")\nprint(\"DONE!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Évaluation\nfrom sentence_transformers.util import cos_sim\nimport random\n\nfinetuned = SentenceTransformer(\"minilm-chess-arbiter-fr\")\ntest_samples = random.sample(triplets, 10)\n\nprint(\"=\" * 50)\nprint(\"ÉVALUATION\")\nprint(\"=\" * 50)\n\ncorrect = 0\nfor i, t in enumerate(test_samples):\n    q = finetuned.encode(t[\"anchor\"])\n    p = finetuned.encode(t[\"positive\"])\n    n = finetuned.encode(t[\"negative\"])\n    \n    ok = cos_sim(q, p).item() > cos_sim(q, n).item()\n    correct += int(ok)\n    print(f\"[{i+1}] {'OK' if ok else 'FAIL'}\")\n\nprint(\"=\" * 50)\nprint(f\"Score: {correct}/10 = {correct*10}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Télécharger\nimport shutil\nfrom google.colab import files\nshutil.make_archive(\"minilm-chess-arbiter-fr\", \"zip\", \"minilm-chess-arbiter-fr\")\nfiles.download(\"minilm-chess-arbiter-fr.zip\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
