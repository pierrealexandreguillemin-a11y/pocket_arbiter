# Analyse ISO - ModÃ¨le Fine-TunÃ© et DÃ©ploiement Android

> **Document**: ISO 25010 / ISO 42001 - Analyse de ConformitÃ©
> **Version**: 2.0
> **Date**: 2026-01-18
> **Auteur**: Claude Code Assistant
> **Statut**: SOLUTIONS SIMPLIFIÃ‰ES VALIDÃ‰ES

---

## 1. RÃ©sumÃ© ExÃ©cutif

### 1.1 Contexte Applicatif

**Pocket Arbiter** : Application RAG mobile pour arbitres d'Ã©checs.

| Corpus | Contenu | Langue | Statut ModÃ¨le |
|--------|---------|--------|---------------|
| **FR** | 29 PDF FFE (rÃ¨glements franÃ§ais) | FranÃ§ais | Fine-tunÃ© âœ… |
| **INTL** | 1 PDF FIDE (Laws of Chess) | Anglais | Base multilingue |

### 1.2 ModÃ¨le Fine-TunÃ© Actuel

- **PrÃ©cision Ã©valuation** : 100% (10/10 sur triplets test)
- **Localisation** : [Pierrax/embeddinggemma-chess-arbiter-fr](https://huggingface.co/Pierrax/embeddinggemma-chess-arbiter-fr)
- **Taille** : 1.21 GB (format safetensors FP32)

### 1.3 ProblÃ¨me Initial

| CritÃ¨re | Valeur Actuelle | Cible Android | ConformitÃ© |
|---------|-----------------|---------------|------------|
| Taille modÃ¨le | 1.21 GB | < 200 MB | âŒ NON CONFORME |
| Format | safetensors | TFLite | âŒ NON CONFORME |
| RAM requise | ~2-4 GB | < 500 MB | âŒ NON CONFORME |

### 1.4 Simplification ClÃ© : Choix du Corpus AVANT Query

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE SIMPLIFIÃ‰E                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Ã‰TAPE 1: Choix Corpus (UI)      Ã‰TAPE 2: Query            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  ğŸ‡«ğŸ‡· FR   â”‚  â”‚  ğŸŒ INTL â”‚  â†’   â”‚ "Temps rÃ©flexion â”‚      â”‚
â”‚   â”‚ (29 PDF) â”‚  â”‚  (FIDE)  â”‚      â”‚  cadence rapide" â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚        â”‚             â”‚                     â”‚                â”‚
â”‚        â–¼             â–¼                     â–¼                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚Model FR  â”‚  â”‚Model INTLâ”‚  â†’   â”‚   RAG Pipeline   â”‚      â”‚
â”‚   â”‚(fine-tunÃ©â”‚  â”‚ (base)   â”‚      â”‚   + RÃ©ponse      â”‚      â”‚
â”‚   â”‚ ~180 MB) â”‚  â”‚ ~179 MB) â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Impact de cette architecture** :

| Aspect | Switch Dynamique (rejetÃ©) | Choix PrÃ©alable (adoptÃ©) |
|--------|---------------------------|--------------------------|
| Latence overhead | +250-950% âŒ | **0%** âœ… |
| ComplexitÃ© runtime | Adapters dynamiques | Load unique |
| RAM | Base + adapter | 1 seul modÃ¨le |
| ImplÃ©mentation | Complexe | **Simple** |

---

## 2. Cahier des Charges Technique

### 2.1 SpÃ©cifications App Pocket Arbiter

| CaractÃ©ristique | SpÃ©cification |
|-----------------|---------------|
| **Plateforme** | Android 10+ (API 29+) |
| **Cible device** | Mid-range (Snapdragon 7xx, 6-8 GB RAM) |
| **Corpus FR** | 29 PDF FFE, ~500 chunks |
| **Corpus INTL** | 1 PDF FIDE, ~100 chunks |
| **Latence cible** | < 200 ms par query |
| **Stockage max** | < 400 MB total (2 modÃ¨les) |
| **Mode offline** | Obligatoire (arbitrage terrain) |

### 2.2 Workflow Utilisateur

```
1. Arbitre ouvre l'app
2. SÃ©lectionne corpus : "RÃ¨glement FR" ou "FIDE Laws"
   â†’ App charge le modÃ¨le correspondant (1x au switch)
3. Pose sa question
   â†’ Embedding query â†’ Recherche chunks â†’ RÃ©ponse RAG
4. Peut switcher de corpus Ã  tout moment
   â†’ Nouveau chargement modÃ¨le (~1-2 sec)
```

### 2.3 Contraintes ISO

| Norme | Exigence | Impact |
|-------|----------|--------|
| **ISO 25010** | EfficacitÃ© performances | Latence < 200ms, RAM < 500MB |
| **ISO 42001** | TraÃ§abilitÃ© IA | Citations obligatoires, 0% hallucination |
| **ISO 27001** | SÃ©curitÃ© donnÃ©es | Mode offline, pas de cloud |

---

## 3. Solutions SimplifiÃ©es (Choix Corpus PrÃ©alable)

### 3.1 Architecture Retenue : 2 ModÃ¨les TFLite SÃ©parÃ©s

| ModÃ¨le | Source | Quantization | Taille | Usage |
|--------|--------|--------------|--------|-------|
| `embeddinggemma_fr.tflite` | Fine-tunÃ© FR â†’ PTQ/QAT | Mixed INT4/INT8 | ~180 MB | Corpus FR |
| `embeddinggemma_intl.tflite` | litert-community | Mixed INT4/INT8 | 179 MB | Corpus INTL |

**Stockage total** : ~360 MB (conforme < 400 MB)

---

### 3.2 Solution A : PTQ Direct (RecommandÃ© - Test Rapide)

**Pour le modÃ¨le FR fine-tunÃ©**

| Aspect | DÃ©tail |
|--------|--------|
| **Temps** | 30-60 minutes |
| **ComplexitÃ©** | â˜…â˜…â˜†â˜†â˜† |
| **Perte qualitÃ© estimÃ©e** | 2-6% |
| **Taille finale** | ~180-250 MB |

**ProcÃ©dure** :

```python
# convert_fr_model.py
import ai_edge_torch
import torch
from sentence_transformers import SentenceTransformer

# 1. Charger le modÃ¨le fine-tunÃ©
model = SentenceTransformer("Pierrax/embeddinggemma-chess-arbiter-fr")
model.eval()

# 2. Extraire le transformer
transformer = model[0].auto_model

# 3. Exemple input (seq_length=256 pour mobile)
example_input = torch.randint(0, 256000, (1, 256))
attention_mask = torch.ones(1, 256, dtype=torch.long)

# 4. Conversion avec quantization INT8
from ai_edge_torch.quantize import quant_config

edge_model = ai_edge_torch.convert(
    transformer,
    (example_input, attention_mask),
    quant_config=quant_config.QuantConfig(mode="dynamic_int8")
)

# 5. Export
edge_model.export("models/embeddinggemma_fr.tflite")
print("Export OK: models/embeddinggemma_fr.tflite")
```

**Validation qualitÃ©** :

```python
# validate_quantized.py
import json
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim

# Charger modÃ¨le original et questions gold
original = SentenceTransformer("Pierrax/embeddinggemma-chess-arbiter-fr")
questions = json.load(open("tests/data/questions_gold.json"))

# Comparer embeddings original vs quantized
# (nÃ©cessite inference TFLite - voir annexe)

# CritÃ¨re: perte < 5% sur recall@1
```

---

### 3.3 Solution B : Utiliser litert-community pour INTL

**Pour le corpus INTL (FIDE - anglais)**

| Aspect | DÃ©tail |
|--------|--------|
| **Temps** | 10 minutes (tÃ©lÃ©chargement) |
| **ComplexitÃ©** | â˜…â˜†â˜†â˜†â˜† |
| **QualitÃ©** | Base multilingue (non fine-tunÃ©) |
| **Taille** | 179 MB |

**ProcÃ©dure** :

```bash
# TÃ©lÃ©charger depuis HuggingFace
huggingface-cli download litert-community/embeddinggemma-300m \
    --include "*.tflite" \
    --local-dir models/

# Renommer pour clartÃ©
mv models/embeddinggemma_seq256.tflite models/embeddinggemma_intl.tflite
```

**Note** : Le modÃ¨le base EmbeddingGemma est multilingue et performant sur l'anglais sans fine-tuning spÃ©cifique.

---

### 3.4 Solution C : QAT si PTQ Insuffisant

**Si la perte de qualitÃ© PTQ > 5%**

| Aspect | DÃ©tail |
|--------|--------|
| **Temps** | 4-8 heures |
| **ComplexitÃ©** | â˜…â˜…â˜…â˜†â˜† |
| **Perte qualitÃ©** | 1-3% |
| **Taille finale** | 150-200 MB |

**ProcÃ©dure** :

```python
# qat_finetune.py
import torch
from torch.ao.quantization import get_default_qat_qconfig_mapping
from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer

# 1. Charger modÃ¨le avec QAT config
model = SentenceTransformer("Pierrax/embeddinggemma-chess-arbiter-fr")

# 2. Configurer QAT pour mobile (qnnpack)
qconfig = get_default_qat_qconfig_mapping("qnnpack")

# 3. PrÃ©parer le modÃ¨le
from torch.ao.quantization.quantize_fx import prepare_qat_fx
model_prepared = prepare_qat_fx(model[0].auto_model, qconfig)

# 4. Re-fine-tuner avec les mÃªmes triplets (2-3 epochs suffisent)
# ... (mÃªme code que fine-tuning initial)

# 5. Convertir et exporter
from torch.ao.quantization.quantize_fx import convert_fx
model_quantized = convert_fx(model_prepared)
```

---

### 3.5 Solution D : Distillation MiniLM (Option LÃ©gÃ¨re)

**Pour rÃ©duire encore la taille (< 100 MB)**

| Aspect | DÃ©tail |
|--------|--------|
| **Temps** | 2-6 heures |
| **ComplexitÃ©** | â˜…â˜…â˜…â˜†â˜† |
| **Perte qualitÃ©** | 3-8% |
| **Taille finale** | 50-80 MB |

**ProcÃ©dure** :

```python
# distill_to_minilm.py
from sentence_transformers import SentenceTransformer, losses, InputExample
from torch.utils.data import DataLoader
import json

# Teacher: modÃ¨le fine-tunÃ©
teacher = SentenceTransformer("Pierrax/embeddinggemma-chess-arbiter-fr")

# Student: MiniLM compact
student = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# Charger tous les chunks du corpus FR
chunks = json.load(open("corpus/processed/chunks_fr.json"))
texts = [c["text"] for c in chunks]

# GÃ©nÃ©rer embeddings teacher
print(f"GÃ©nÃ©ration embeddings teacher pour {len(texts)} chunks...")
teacher_embeddings = teacher.encode(texts, convert_to_tensor=True, show_progress_bar=True)

# Dataset de distillation
train_examples = [
    InputExample(texts=[text], label=emb.tolist())
    for text, emb in zip(texts, teacher_embeddings)
]

# EntraÃ®ner student
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.MSELoss(model=student)

student.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=3,
    warmup_steps=100,
    output_path="models/minilm-chess-fr-distilled"
)

# Convertir vers TFLite
# ... (mÃªme procÃ©dure que Solution A)
```

**Avantage** : ModÃ¨le final ~80 MB, infÃ©rence ~30-50ms

**InconvÃ©nient** : Dimensions diffÃ©rentes (384 vs 768), nÃ©cessite re-indexation corpus

---

## 4. Matrice de DÃ©cision SimplifiÃ©e

### 4.1 Pour le Corpus FR (fine-tunÃ©)

| Solution | Temps | Taille | QualitÃ© | Recommandation |
|----------|-------|--------|---------|----------------|
| **A. PTQ** | 30 min | ~200 MB | â˜…â˜…â˜…â˜† | **ESSAYER EN 1ER** |
| **C. QAT** | 4-8h | ~180 MB | â˜…â˜…â˜…â˜… | Si PTQ perte > 5% |
| **D. Distillation** | 2-6h | ~80 MB | â˜…â˜…â˜…â˜† | Si contrainte taille |

### 4.2 Pour le Corpus INTL (FIDE)

| Solution | Temps | Taille | QualitÃ© | Recommandation |
|----------|-------|--------|---------|----------------|
| **B. litert-community** | 10 min | 179 MB | â˜…â˜…â˜…â˜† | **UTILISER DIRECTEMENT** |
| Fine-tuning INTL | 4-10h | ~180 MB | â˜…â˜…â˜…â˜… | Si qualitÃ© insuffisante |

---

## 5. Plan d'Action Final

### Phase 1 : DÃ©ploiement Rapide (1-2 heures)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 1.1: ModÃ¨le INTL                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ TÃ©lÃ©charger litert-community/embeddinggemma-300m          â”‚
â”‚ â€¢ Copier vers models/embeddinggemma_intl.tflite             â”‚
â”‚ â€¢ Temps: 10 minutes                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 1.2: ModÃ¨le FR (PTQ)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Convertir Pierrax/embeddinggemma-chess-arbiter-fr         â”‚
â”‚ â€¢ PTQ INT8 avec ai-edge-torch                               â”‚
â”‚ â€¢ Export models/embeddinggemma_fr.tflite                    â”‚
â”‚ â€¢ Temps: 30-60 minutes                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ã‰TAPE 1.3: Validation                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Tester sur questions gold standard                        â”‚
â”‚ â€¢ Mesurer recall@1, recall@3                                â”‚
â”‚ â€¢ CritÃ¨re: perte < 5% vs modÃ¨le FP32                        â”‚
â”‚ â€¢ Temps: 30 minutes                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2 : Optimisation (si nÃ©cessaire)

```
SI perte > 5% sur FR:
    â†’ Solution C: QAT re-fine-tuning (4-8h)

SI contrainte taille < 100MB:
    â†’ Solution D: Distillation MiniLM (2-6h)

SI qualitÃ© INTL insuffisante:
    â†’ Fine-tuning INTL avec triplets FIDE (4-10h)
```

### Phase 3 : IntÃ©gration Android

```kotlin
// ChessArbiterApp.kt
class EmbeddingManager(context: Context) {
    private var currentModel: Interpreter? = null
    private var currentCorpus: Corpus = Corpus.FR

    enum class Corpus { FR, INTL }

    fun switchCorpus(corpus: Corpus) {
        currentModel?.close()
        val modelPath = when (corpus) {
            Corpus.FR -> "embeddinggemma_fr.tflite"
            Corpus.INTL -> "embeddinggemma_intl.tflite"
        }
        currentModel = Interpreter(loadModelFile(modelPath))
        currentCorpus = corpus
    }

    fun embed(text: String): FloatArray {
        // Tokenize + inference
        return currentModel!!.runInference(tokenize(text))
    }
}
```

---

## 6. Livrables Attendus

| Fichier | Taille | Source | Statut |
|---------|--------|--------|--------|
| `models/embeddinggemma_fr.tflite` | ~180 MB | PTQ du fine-tunÃ© | Ã€ CRÃ‰ER |
| `models/embeddinggemma_intl.tflite` | 179 MB | litert-community | Ã€ TÃ‰LÃ‰CHARGER |
| `app/src/main/assets/` | ~360 MB | Copie des modÃ¨les | Ã€ INTÃ‰GRER |

---

## 7. RÃ©fÃ©rences

### Documentation Officielle
- [Google AI Edge - LiteRT](https://ai.google.dev/edge/litert)
- [EmbeddingGemma Overview](https://ai.google.dev/gemma/docs/embeddinggemma)
- [ai-edge-torch GitHub](https://github.com/google-ai-edge/ai-edge-torch)
- [LiteRT Semantic Similarity Sample](https://github.com/google-ai-edge/LiteRT/tree/main/litert/samples/semantic_similarity)

### ModÃ¨les
- [Pierrax/embeddinggemma-chess-arbiter-fr](https://huggingface.co/Pierrax/embeddinggemma-chess-arbiter-fr) - Fine-tunÃ© FR (100% eval)
- [litert-community/embeddinggemma-300m](https://huggingface.co/litert-community/embeddinggemma-300m) - TFLite prÃªt (179 MB)
- [google/embeddinggemma-300m](https://huggingface.co/google/embeddinggemma-300m) - Base originale

### Papers & Articles
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314)
- [MiniLM: Deep Self-Attention Distillation](https://arxiv.org/pdf/2002.10957)
- [LoRA-Switch Latency Analysis](https://arxiv.org/html/2405.17741v1) - Justifie le choix prÃ©alable vs switch dynamique

### Tutoriels
- [Sentence Transformers Distillation](https://sbert.net/examples/sentence_transformer/training/distillation/README.html)
- [PyTorch QAT Guide](https://pytorch.org/blog/quantization-aware-training/)
- [Accelerate Sentence Transformers with Optimum](https://www.philschmid.de/optimize-sentence-transformers)

---

## 8. Annexes

### A. Benchmark litert-community (Samsung S25 Ultra)

| Backend | Seq 256 | Seq 512 | Memory |
|---------|---------|---------|--------|
| GPU Mixed | 64 ms | 119 ms | 762 MB |
| CPU 4T XNNPACK | 66 ms | 169 ms | 110 MB |

**Recommandation** : Utiliser seq_length=256 pour latence optimale sur mid-range.

### B. Checklist Validation ISO

- [ ] **ISO 25010** : Latence < 200ms mesurÃ©e sur device cible
- [ ] **ISO 25010** : RAM < 500MB en pic
- [ ] **ISO 42001** : Recall@1 > 80% sur questions gold
- [ ] **ISO 42001** : 0 hallucination (citations vÃ©rifiables)
- [ ] **ISO 27001** : Mode offline fonctionnel (pas de requÃªte rÃ©seau)

### C. Script de Test Complet

```bash
#!/bin/bash
# test_deployment.sh

echo "=== Phase 1: TÃ©lÃ©chargement INTL ==="
huggingface-cli download litert-community/embeddinggemma-300m \
    --include "*seq256*.tflite" \
    --local-dir models/

echo "=== Phase 2: Conversion FR ==="
python scripts/convert_fr_model.py

echo "=== Phase 3: Validation ==="
python scripts/validate_models.py

echo "=== RÃ©sultats ==="
ls -lh models/*.tflite
```

---

**Document mis Ã  jour le 2026-01-18**
**Version 2.0 - Architecture simplifiÃ©e (choix corpus prÃ©alable)**
**Conforme ISO 25010, ISO 42001, ISO 12207**
