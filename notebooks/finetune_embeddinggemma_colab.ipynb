{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": "# Installation des dépendances\n%pip install -q \"sentence-transformers[train]\" datasets accelerate huggingface_hub transformers>=4.47.0\n\nfrom google.colab import userdata\nfrom huggingface_hub import login\nlogin(token=userdata.get('HF_TOKEN'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data"
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/pierrealexandreguillemin-a11y/pocket_arbiter/main/data/training/triplets_training.jsonl\n",
    "import json\n",
    "triplets = [json.loads(l) for l in open(\"triplets_training.jsonl\") if l.strip()]\n",
    "print(f\"Triplets: {len(triplets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from datasets import Dataset\n",
    "\n",
    "# Nettoyer la VRAM\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Vérifier GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Charger le modèle EmbeddingGemma 300M\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300M\", device=device)\n",
    "print(f\"Model: {model.get_sentence_embedding_dimension()} dims\")\n",
    "\n",
    "# Activer gradient checkpointing pour économiser la VRAM\n",
    "model[0].auto_model.gradient_checkpointing_enable()\n",
    "\n",
    "# Configuration ULTRA optimisée pour T4 15GB (OOM fix)\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=SentenceTransformerTrainingArguments(\n",
    "        output_dir=\"embeddinggemma-chess-arbiter-fr\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,            # MINIMAL pour T4\n",
    "        gradient_accumulation_steps=16,           # Effective batch = 16\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=False,                               # EmbeddingGemma ne supporte pas fp16\n",
    "        bf16=False,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "        dataloader_drop_last=True,\n",
    "        optim=\"adamw_torch_fused\",                # Optimizer plus efficace en mémoire\n",
    "    ),\n",
    "    train_dataset=Dataset.from_list(triplets),\n",
    "    loss=MultipleNegativesRankingLoss(model)\n",
    ")\n",
    "\n",
    "print(\"Training...\")\n",
    "trainer.train()\n",
    "model.save(\"embeddinggemma-chess-arbiter-fr\")\n",
    "print(\"Training terminé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation rapide du modèle fine-tuné (conformité ISO 42001)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "import random\n",
    "\n",
    "# Charger le modèle fine-tuné\n",
    "finetuned = SentenceTransformer(\"embeddinggemma-chess-arbiter-fr\")\n",
    "\n",
    "# Échantillon de test (5 triplets aléatoires)\n",
    "test_samples = random.sample(triplets, min(5, len(triplets)))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ÉVALUATION QUALITÉ (ISO 42001 - AI-R03: Recall)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correct = 0\n",
    "for i, t in enumerate(test_samples):\n",
    "    q_emb = finetuned.encode(t[\"anchor\"])\n",
    "    pos_emb = finetuned.encode(t[\"positive\"])\n",
    "    neg_emb = finetuned.encode(t[\"negative\"])\n",
    "    \n",
    "    sim_pos = cos_sim(q_emb, pos_emb).item()\n",
    "    sim_neg = cos_sim(q_emb, neg_emb).item()\n",
    "    \n",
    "    is_correct = sim_pos > sim_neg\n",
    "    correct += int(is_correct)\n",
    "    \n",
    "    print(f\"\\n[{i+1}] Question: {t['anchor'][:60]}...\")\n",
    "    print(f\"    Sim(positive): {sim_pos:.4f}\")\n",
    "    print(f\"    Sim(negative): {sim_neg:.4f}\")\n",
    "    print(f\"    {'✅ CORRECT' if is_correct else '❌ INCORRECT'}\")\n",
    "\n",
    "accuracy = correct / len(test_samples) * 100\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"RÉSULTAT: {correct}/{len(test_samples)} = {accuracy:.0f}%\")\n",
    "print(f\"Cible ISO: ≥80%  →  {'✅ CONFORME' if accuracy >= 80 else '⚠️ À AMÉLIORER'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "shutil.make_archive(\"embeddinggemma-chess-arbiter-fr\", \"zip\", \"embeddinggemma-chess-arbiter-fr\")\n",
    "files.download(\"embeddinggemma-chess-arbiter-fr.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
