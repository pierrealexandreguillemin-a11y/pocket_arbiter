# Analyse ISO - Pipeline RAG Complet et DÃ©ploiement Android

> **Document**: ISO 25010 / ISO 42001 - Analyse de ConformitÃ©
> **Version**: 3.0
> **Date**: 2026-01-18
> **Auteur**: Claude Code Assistant
> **Statut**: ARCHITECTURE RAG COMPLÃˆTE VALIDÃ‰E

---

## 1. RÃ©sumÃ© ExÃ©cutif

### 1.1 Contexte Applicatif

**Pocket Arbiter** : Application RAG mobile 100% offline pour arbitres d'Ã©checs.

| Corpus | Contenu | Chunks | Langue |
|--------|---------|--------|--------|
| **FR** | 29 PDF FFE | ~2794 | FranÃ§ais |
| **INTL** | 1 PDF FIDE | ~100 | Anglais |

### 1.2 Pipeline RAG Complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INDEXATION (offline, 1x)                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PDFs â”€â”€â†’ Extraction â”€â”€â†’ Chunking â”€â”€â†’ Embedding â”€â”€â†’ SQLite + FTS5          â”‚
â”‚  30 docs   PyMuPDF       400 tokens   768D          corpus.db               â”‚
â”‚                          ~2900 chunks  EmbeddingGemma                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RETRIEVAL (runtime mobile)                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query â”€â”€â†’ Embedding â”€â”€â†’ Hybrid Search â”€â”€â†’ Top-5 chunks                    â”‚
â”‚            768D query    70% BM25 + 30% vector                              â”‚
â”‚            ~60-170ms     SQLite FTS5 + cosine                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GENERATION (runtime mobile)                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Top-5 + Query â”€â”€â†’ LLM â”€â”€â†’ RÃ©ponse + Citations verbatim                    â”‚
â”‚                    Gemma 3 270M TFLite                                      â”‚
â”‚                    ~2-4 sec                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ModÃ¨le Fine-TunÃ© Actuel

- **PrÃ©cision Ã©valuation** : 100% (10/10 sur triplets test)
- **Localisation** : [Pierrax/embeddinggemma-chess-arbiter-fr](https://huggingface.co/Pierrax/embeddinggemma-chess-arbiter-fr)
- **Taille** : 1.21 GB (format safetensors FP32) â†’ **Trop gros pour mobile**

---

## 2. Contraintes et Budget

### 2.1 Contraintes CDC (sources: VISION.md, ARCHITECTURE.md)

| Contrainte | Valeur | Source | CriticitÃ© |
|------------|--------|--------|-----------|
| Mode | 100% offline | VISION.md | BLOQUANT |
| Plateforme | Android 10+ (API 29+) | VISION.md | BLOQUANT |
| RAM max | 500 MB | ARCHITECTURE.md | BLOQUANT |
| **Assets max** | **500 MB** | ARCHITECTURE.md | **BLOQUANT** |
| APK max | 100 MB | ARCHITECTURE.md | IMPORTANT |
| Latence totale | < 5 secondes | VISION.md | IMPORTANT |
| Recall retrieval | >= 80% | QUALITY_REQ | BLOQUANT |
| Hallucination | 0% | ISO 42001 | BLOQUANT |

### 2.2 Analyse Budget Assets

#### ScÃ©nario A : 2 Embeddings SÃ©parÃ©s (REJETÃ‰)

| Composant | Taille | Conforme ? |
|-----------|--------|------------|
| Embedding FR (fine-tunÃ©, quantized) | ~180 MB | - |
| Embedding INTL (litert-community) | 179 MB | - |
| LLM Gemma 3 270M | ~200 MB | - |
| **TOTAL** | **559 MB** | âŒ > 500 MB |

#### ScÃ©nario B : 1 Embedding PartagÃ© (RECOMMANDÃ‰)

| Composant | Taille | Conforme ? |
|-----------|--------|------------|
| Embedding unique (multilingue) | ~180 MB | - |
| LLM Gemma 3 270M | ~200 MB | - |
| Index SQLite + FTS5 | ~20 MB | - |
| **TOTAL** | **~400 MB** | âœ… < 500 MB |

#### ScÃ©nario C : MiniLM DistillÃ© (OPTIMAL TAILLE)

| Composant | Taille | Conforme ? |
|-----------|--------|------------|
| MiniLM distillÃ© | ~80 MB | - |
| LLM Gemma 3 270M | ~200 MB | - |
| Index SQLite + FTS5 | ~20 MB | - |
| **TOTAL** | **~300 MB** | âœ… < 500 MB |

### 2.3 Analyse Budget RAM

| Composant | RAM (CPU) | RAM (GPU) |
|-----------|-----------|-----------|
| Embedding EmbeddingGemma | 110 MB | 762 MB |
| LLM Gemma 3 270M | ~150 MB | ~300 MB |
| App + OS overhead | ~100 MB | ~100 MB |
| **TOTAL** | **~360 MB** | **~1.1 GB** |

**Recommandation** : Utiliser CPU (XNNPACK) sur mid-range, GPU optionnel sur flagship.

---

## 3. Architecture SimplifiÃ©e : Choix Corpus AVANT Query

### 3.1 Principe ClÃ©

L'utilisateur sÃ©lectionne le corpus (FR ou INTL) **avant** de poser sa question.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WORKFLOW UTILISATEUR                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   Ã‰TAPE 1: Choix Corpus        Ã‰TAPE 2: Question                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚  ğŸ‡«ğŸ‡· FR   â”‚  â”‚  ğŸŒ INTL â”‚   â”‚ "Temps de rÃ©flexion   â”‚       â”‚
â”‚   â”‚ (29 PDF) â”‚  â”‚  (FIDE)  â”‚   â”‚  en cadence rapide ?" â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚        â”‚             â”‚                     â”‚                    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                     â”‚                    â”‚
â”‚               â–¼                            â–¼                    â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚        â”‚ Load Index â”‚              â”‚ RAG Pipeline â”‚             â”‚
â”‚        â”‚ corpus.db  â”‚              â”‚ â†’ RÃ©ponse    â”‚             â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Avantage : Pas de Switch Dynamique

| Aspect | Switch Dynamique (rejetÃ©) | Choix PrÃ©alable (adoptÃ©) |
|--------|---------------------------|--------------------------|
| Latence overhead | +250-950% | **0%** |
| ComplexitÃ© | Adapters runtime | Load unique |
| RAM | Base + adapter | 1 modÃ¨le |
| ImplÃ©mentation | Complexe | **Simple** |

---

## 4. Solutions RecommandÃ©es

### 4.1 Solution Optimale : Embedding Unique Multi-Corpus

**Principe** : Fine-tuner UN SEUL modÃ¨le EmbeddingGemma sur les 2 corpus (FR + INTL).

| Aspect | DÃ©tail |
|--------|--------|
| **Temps** | 6-12 heures (fine-tuning combinÃ©) |
| **ComplexitÃ©** | â˜…â˜…â˜…â˜†â˜† |
| **Taille finale** | ~180 MB (embedding) + ~200 MB (LLM) = **~380 MB** |
| **QualitÃ©** | OptimisÃ© pour les 2 corpus |

**ProcÃ©dure** :

```python
# 1. GÃ©nÃ©rer triplets pour les 2 corpus
triplets_fr = load_triplets("data/training/triplets_fr.jsonl")
triplets_intl = load_triplets("data/training/triplets_intl.jsonl")
triplets_combined = triplets_fr + triplets_intl

# 2. Fine-tuner sur donnÃ©es combinÃ©es
trainer = SentenceTransformerTrainer(
    model=model,
    train_dataset=Dataset.from_list(triplets_combined),
    loss=MultipleNegativesRankingLoss(model)
)
trainer.train()

# 3. Exporter en TFLite
# ... (ai-edge-torch PTQ/QAT)
```

**Avantages** :
- âœ… UN seul modÃ¨le embedding pour FR et INTL
- âœ… Budget respectÃ© (~380 MB total)
- âœ… Fine-tuning spÃ©cifique domaine Ã©checs
- âœ… Meilleure qualitÃ© que base multilingue gÃ©nÃ©rique

---

### 4.2 Solution Rapide : Base Multilingue (Sans Fine-Tuning)

**Principe** : Utiliser litert-community/embeddinggemma-300m directement.

| Aspect | DÃ©tail |
|--------|--------|
| **Temps** | 1 heure (tÃ©lÃ©chargement + intÃ©gration) |
| **ComplexitÃ©** | â˜…â˜†â˜†â˜†â˜† |
| **Taille finale** | 179 MB + ~200 MB = **~379 MB** |
| **QualitÃ©** | Base multilingue (non optimisÃ© domaine) |

**ProcÃ©dure** :

```bash
# TÃ©lÃ©charger modÃ¨le TFLite prÃªt
huggingface-cli download litert-community/embeddinggemma-300m \
    --include "*seq256*.tflite" \
    --local-dir models/
```

**Avantages** :
- âœ… ImmÃ©diatement disponible
- âœ… DÃ©jÃ  quantifiÃ© (mixed INT4/INT8)
- âœ… TestÃ© sur mobile (Samsung S25 Ultra)

**InconvÃ©nients** :
- âŒ Perte du fine-tuning FR (100% â†’ ~70-80% recall estimÃ©)
- âŒ Moins prÃ©cis sur terminologie Ã©checs franÃ§aise

---

### 4.3 Solution Ultra-LÃ©gÃ¨re : Distillation MiniLM

**Principe** : Distiller les connaissances du modÃ¨le fine-tunÃ© vers MiniLM.

| Aspect | DÃ©tail |
|--------|--------|
| **Temps** | 2-6 heures |
| **ComplexitÃ©** | â˜…â˜…â˜…â˜†â˜† |
| **Taille finale** | ~80 MB + ~200 MB = **~280 MB** |
| **QualitÃ©** | 90-97% du teacher |

**ProcÃ©dure** :

```python
from sentence_transformers import SentenceTransformer, losses

# Teacher: modÃ¨le fine-tunÃ© (ou base)
teacher = SentenceTransformer("Pierrax/embeddinggemma-chess-arbiter-fr")

# Student: MiniLM multilingue
student = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# Distillation sur tous les chunks (FR + INTL)
chunks_all = load_all_chunks()
teacher_embeddings = teacher.encode(chunks_all)

# ... training distillation ...
```

**Avantages** :
- âœ… ModÃ¨le trÃ¨s compact (~80 MB)
- âœ… InfÃ©rence rapide (~30-50 ms)
- âœ… Maximum de marge pour le LLM

**InconvÃ©nients** :
- âŒ Dimensions diffÃ©rentes (384 vs 768)
- âŒ NÃ©cessite rÃ©-indexation corpus

---

## 5. Composant LLM (GÃ©nÃ©ration)

### 5.1 Options LLM pour Mobile

| ModÃ¨le | Taille | RAM | Latence | QualitÃ© |
|--------|--------|-----|---------|---------|
| **Gemma 3 270M** | ~200 MB | ~150 MB | ~2-4s | â˜…â˜…â˜…â˜† |
| Gemma 3 1B | ~600 MB | ~400 MB | ~5-8s | â˜…â˜…â˜…â˜… |
| Phi-3.5-mini | ~500 MB | ~350 MB | ~4-6s | â˜…â˜…â˜…â˜… |

**Recommandation** : Gemma 3 270M pour respecter budget 500 MB.

### 5.2 IntÃ©gration LLM

```kotlin
// Android - MediaPipe GenAI
class LLMEngine(context: Context) {
    private val llmInference = LlmInference.createFromOptions(
        context,
        LlmInference.LlmInferenceOptions.builder()
            .setModelPath("gemma3_270m.tflite")
            .setMaxTokens(512)
            .build()
    )

    fun generate(prompt: String): String {
        return llmInference.generateResponse(prompt)
    }
}
```

### 5.3 Template Prompt RAG

```
Tu es un assistant pour arbitres d'Ã©checs. RÃ©ponds UNIQUEMENT en te basant sur les extraits fournis.

EXTRAITS DU RÃˆGLEMENT:
{chunks}

QUESTION: {query}

INSTRUCTIONS:
- Cite le texte exact entre guillemets
- Indique la source (document, page)
- Si non trouvÃ©, dis "Information non trouvÃ©e dans les extraits"
- Ne jamais inventer d'information

RÃ‰PONSE:
```

---

## 6. Matrice de DÃ©cision Finale

| Solution | Temps | Taille Totale | QualitÃ© | Recommandation |
|----------|-------|---------------|---------|----------------|
| **4.1 Fine-tuning unique** | 6-12h | ~380 MB | â˜…â˜…â˜…â˜…â˜… | **OPTIMAL** |
| **4.2 Base multilingue** | 1h | ~379 MB | â˜…â˜…â˜…â˜† | RAPIDE |
| **4.3 Distillation MiniLM** | 2-6h | ~280 MB | â˜…â˜…â˜…â˜…â˜† | ULTRA-LÃ‰GER |

---

## 7. Plan d'Action

### Phase 1 : DÃ©ploiement Rapide (1-2 heures)

```
1. TÃ©lÃ©charger litert-community/embeddinggemma-300m (179 MB)
2. TÃ©lÃ©charger Gemma 3 270M TFLite (~200 MB)
3. IntÃ©grer dans app Android
4. Tester recall sur questions gold standard
```

### Phase 2 : Optimisation (si recall < 80%)

```
Option A: Fine-tuning unique FR+INTL (6-12h)
   â†’ Meilleure qualitÃ©, mÃªme taille

Option B: Distillation MiniLM (2-6h)
   â†’ Plus lÃ©ger, marge pour LLM plus gros
```

### Phase 3 : GÃ©nÃ©ration Triplets INTL

```
Si fine-tuning unique choisi:
1. GÃ©nÃ©rer questions synthÃ©tiques sur corpus INTL
2. Hard negative mining
3. Combiner avec triplets FR existants
4. Fine-tuner modÃ¨le combinÃ©
```

---

## 8. Livrables

| Fichier | Taille | Source | Statut |
|---------|--------|--------|--------|
| `models/embeddinggemma.tflite` | ~180 MB | Fine-tuning unique ou litert | Ã€ CRÃ‰ER |
| `models/gemma3_270m.tflite` | ~200 MB | Google AI Edge | Ã€ TÃ‰LÃ‰CHARGER |
| `assets/corpus_fr.db` | ~15 MB | Pipeline indexation | EXISTE |
| `assets/corpus_intl.db` | ~5 MB | Pipeline indexation | Ã€ CRÃ‰ER |
| **TOTAL** | **~400 MB** | - | âœ… < 500 MB |

---

## 9. ConformitÃ© ISO

### 9.1 Checklist

- [ ] **ISO 25010** : Assets < 500 MB
- [ ] **ISO 25010** : RAM < 500 MB en pic
- [ ] **ISO 25010** : Latence < 5s end-to-end
- [ ] **ISO 42001** : Recall >= 80%
- [ ] **ISO 42001** : 0% hallucination (citations obligatoires)
- [ ] **ISO 27001** : 100% offline (pas de requÃªte rÃ©seau)

### 9.2 Tests de Validation

```bash
# Test recall
python scripts/pipeline/tests/test_recall.py --model models/embeddinggemma.tflite

# Test latence
adb shell am start -W com.arbiter/.MainActivity

# Test RAM
adb shell dumpsys meminfo com.arbiter
```

---

## 10. RÃ©fÃ©rences

### Documentation Officielle
- [Google AI Edge - LiteRT](https://ai.google.dev/edge/litert)
- [MediaPipe GenAI](https://ai.google.dev/edge/mediapipe/solutions/genai)
- [EmbeddingGemma](https://ai.google.dev/gemma/docs/embeddinggemma)

### ModÃ¨les
- [litert-community/embeddinggemma-300m](https://huggingface.co/litert-community/embeddinggemma-300m) - TFLite prÃªt
- [Pierrax/embeddinggemma-chess-arbiter-fr](https://huggingface.co/Pierrax/embeddinggemma-chess-arbiter-fr) - Fine-tunÃ© FR

### CDC Projet
- `docs/VISION.md` - Vision et contraintes
- `docs/ARCHITECTURE.md` - Architecture technique
- `docs/RETRIEVAL_PIPELINE.md` - Pipeline RAG dÃ©taillÃ©
- `docs/QUALITY_REQUIREMENTS.md` - Exigences qualitÃ©

---

**Document mis Ã  jour le 2026-01-18**
**Version 3.0 - Pipeline RAG complet (Embedding + LLM)**
**Conforme ISO 25010, ISO 42001, ISO 12207, ISO 27001**
